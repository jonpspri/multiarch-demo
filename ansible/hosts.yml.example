#
#  Edit this file to reflect the appropriate SSH certificates and hosts.  It
#  almost certainly will not work as currently set up.
#

#
#  Do the needful to get ansible to work.  This should work with most modern
#  cloud providers on an Ubuntu image.  The private key should be configured
#  for administrator login by your cloud provider.  Most cloud providers have
#  that initial login for root (despite Ubuntu policy.  Sigh.)  See the note
#  about setting 'ansible_user' for individual images below.
#
all:
  vars:
    ansible_python_interpreter: /usr/bin/python3
    ansible_ssh_private_key_file: ~/.ssh/openwhisk-key.pem
    ansible_user: root

    #
    #  This is the PUBLIC key for the key Jenkins will use to log in as
    #  'jenkins_slave' on the slave instances.
    #
    #  TODO:  Better name for this file?  Also, maybe it should be a
    #         variable only under masters.  But that's for another commit.
    #
    ssh_public_key_file: "~/.ssh/openwhisk-key.pub"
    #
    #  The slaves need to be able to log in to one or more docker
    #  registries to upload images.
    #
    docker_logins:
    - registry: docker.io
      username: username_here
      password: plaintext_password_here
    - registry: https://index.docker.io/v1/
      username: username_here
      password: plaintext_password_here
    - registry: us.icr.io
      username: iamapikey
      password: plaintext_iam_api_key_here


#
#  This is where the master node(s) is set up.
#
#  TODO:  More Jenkins configuration should be automatic on this node than
#         currently is.  Also, SSL configuration (including LetsEncrypt)
#         could be automated.
#
masters:
  hosts:
    jenkins-master.example.com:

#
#  Demo servers
#
#  The first four hosts from this list will be used by the
#  bin/4panes.sh script to demonstrate multi-architecture in tmux panes.
#  'ansible_user' will be used if a privileged user is requested with
#  'bin/4panes.sh -p'.
#
demo_servers:
  hosts:
    x86-64-a.example.com:
    x86-64-b.example.com:
    ppc64le.example.com:
      ansible_user: ubuntu
    s390x.example.com:
      ansible_user: ubuntu

#
#  These nodes are to be configured as slaves (see slaves.yml).
#  Note that for nodes where you do not have direct access, you
#  can set an 'ansible_user' to an ID that can elevate privileges.
#
docker_slaves:
  hosts: &docker_slaves
    x86-64-a.example.com:
    x86-64-b.example.com:
    ppc64le.example.com:
      ansible_user: ubuntu
    s390x.example.com:
      ansible_user: ubuntu
    aarch64-a.example.com:
    aarch64-big.example.com:
podman_slaves:
  hosts: *docker_slaves


docker_tls_hosts:
  hosts:
    x86-64-a.example.com:
    x86-64-b.example.com:
    ppc64le.example.com:
      ansible_user: ubuntu
    s390x.example.com:
      ansible_user: ubuntu
    aarch64-a.example.com:
    aarch64-big.example.com:
  vars:
    #
    #  Information about the CA to be used to create server certificates.  It
    #  is exepected you will use the same CA to create client certificates
    #  for connection to the docker engines.
    #
    ownca_path: /Users/username/.docker/tls/ca.pem
    ownca_privatekey_path: /Users/username/.docker/tls/ca-key.pem
    ownca_privatekey_passphrase: "plaintext_passphrase_here"

    #
    #  This will be used as a working/archive directory to store the
    #  server certificates.  I like to keep everything in one place,
    #  but you could you a temporary/scratch directory if you prefer
    #
    docker_tls_root: /Users/username/.docker/tls
